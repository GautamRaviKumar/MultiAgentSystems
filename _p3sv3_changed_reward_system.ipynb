{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGag52BXbOaM"
   },
   "source": [
    "# Multi Robot Grid World Assessment part 3\n",
    "**Written by**  \n",
    "Egor Danilov (33411115),  \n",
    "Yash Balchandani(33279950),  \n",
    "Gautam Ravi Kumar(33197970),  \n",
    "Jacob Wicklund (31265936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "XD3VCWYYbOaS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torch import tensor, optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMMR0O7KKFuK"
   },
   "source": [
    "### Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "wha59RLTKFuK"
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple, deque\n",
    "class ReplayBuffer:\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize a ReplayBuffer object.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            action_size (int): Dimensionality of each action.\n",
    "            buffer_size (int): Maximum number of experiences to store in the buffer.\n",
    "            batch_size (int): Number of experiences to sample during training.\n",
    "        \"\"\"\n",
    "        self.action_size = action_size # Maximum possible ways each action can be done.\n",
    "\n",
    "        self.memory = deque(maxlen=buffer_size) #double ended queue for a short term memory of agent.\n",
    "\n",
    "        self.batch_size = batch_size # Resampling of the past experience what is past experience? Named Tuple from part 1.\n",
    "\n",
    "        # (Tagged) named tuple to represent a single experience in the replay memory\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory) # That is the dbl qu length.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to memory.\n",
    "        \"\"\"\n",
    "        exp = self.experience(state, action, reward, next_state, done) # Experience tuple. Defined above.\n",
    "        # Pass in the experience to the short term memory queue.\n",
    "        self.memory.append(exp) # Passes the list of attributes into the memory dbl queue.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def sample(self): #Segway into the deep q network. This is the sampler method.\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences from memory for training.\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size) # Sample all the past experiences to learn the most important qualities so we can delete the experience data.\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device) #retrieve the states from experience stored in double qu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device) # retrieve the actions\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device) # the rewards\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device) # possible next moves, choosing the probabilistic best reward. retrieved from the DQN calculation\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device) # task completed flag. IE shut down.\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones) # send this info back to caller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNEGBDEsKFuL"
   },
   "source": [
    "## Agent Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "xvxZdnClKFuL"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_type, start_position, grid_size):\n",
    "        self.agent_type = agent_type\n",
    "        self.start_position = start_position\n",
    "        self.grid_size = grid_size\n",
    "        self.buff = ReplayBuffer(action_size=4,buffer_size=10000, batch_size=64)\n",
    "        self.picked = False\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "\n",
    "    def action(self, state):\n",
    "        possible_actions = [] # Store legal moves\n",
    "\n",
    "        ### INSERT THE DAIRY QUEEN!\n",
    "\n",
    "\n",
    "        return np.random.choice([0, 1, 2, 3]) #of the legal moves pic one at random to do.\n",
    "\n",
    "    def next_position(self, action): # HERE the 0-3 is passed in and used.\n",
    "            x_disp, y_disp = {\n",
    "                0: (-1, 0), # up\n",
    "                1: (1, 0), #down\n",
    "                2: (0, -1), #left\n",
    "                3: (0, 1) #right\n",
    "                }[action]\n",
    "\n",
    "            return (min(max(self.start_position[0] + x_disp, 0),self.grid_size[0] - 1),\n",
    "                    min(max(self.start_position[1] + y_disp, 0),self.grid_size[0] - 1))# Here is where the boundary is checked.\n",
    "\n",
    "    def move(self, action): # This actually executes the move. Action is passed in and executed.\n",
    "        self.start_position = self.next_position(action)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.buff.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        self.buff.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ImdJqcKKFuM"
   },
   "source": [
    "## Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "GlNwyGBFbOaU"
   },
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "class MultiAgentGridWorld:\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size # Grid world size default to 5.\n",
    "        self.pick_up_position = (0, size - 1) # This puts the box in upper left corner of any size map.\n",
    "        self.delivery_position = (size - 1, 0) # The delivery is in lower right corner for any size map.\n",
    "        self.agents = self.initialize_agents() #This list of tuples is carrying all agent objects\n",
    "        #the world has these attributes.\n",
    "        self.has_package = False\n",
    "        self.pick_reward = 5\n",
    "        self.drop_reward = 5\n",
    "        self.handover_reward = 10\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "    def initialize_agents(self):\n",
    "        # Ensuring agents don't spawn on pickup or delivery positions.\n",
    "        possible_positions = [(i, j)\n",
    "                            for i in range(self.size)\n",
    "                            for j in range(self.size)\n",
    "                            if (i, j) not in [self.pick_up_position, self.delivery_position]]\n",
    "        # Add 2 agents of eachh type\n",
    "        indexes = npr.choice(len(possible_positions), 2, replace=False)\n",
    "        start_positions = [possible_positions[_] for _ in indexes]\n",
    "        return [Agent(\"pickup\", start_positions[0], (self.size, self.size)),\n",
    "                    Agent(\"delivery\", start_positions[1], (self.size, self.size))]\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def step(self, actions): # so this method needs a bunch of actions.\n",
    "        memories = [a.start_position for a in self.agents]\n",
    "        self.steps += 1\n",
    "        \n",
    "        pick_agents = [agent for agent in self.agents if agent.agent_type == \"pickup\" and agent.done == False]\n",
    "        drop_agents = [agent for agent in self.agents if agent.agent_type == \"delivery\" and agent.done == False]\n",
    "\n",
    "        for agent in pick_agents: # here i need the entire list of actions\n",
    "            if agent.start_position == self.pick_up_position and agent.picked == False:\n",
    "                agent.picked = True\n",
    "                agent.reward += self.pick_reward - self.steps**2\n",
    "            for agent_2 in drop_agents:\n",
    "                if agent.start_position == agent_2.start_position and agent.picked == False and agent_2.picked == True:\n",
    "                    agent.picked = False\n",
    "                    agent_2.picked = True\n",
    "                    agent.reward += self.handover_reward - self.steps**2\n",
    "                    agent_2.reward += self.handover_reward - self.steps**2\n",
    "                    agent.done = True\n",
    "                    \n",
    "                    \n",
    "        for agent in drop_agents: # here i need the entire list of actions\n",
    "            if agent.start_position == self.pick_up_position and agent.picked == True:\n",
    "                agent.reward += self.drop_reward - self.steps**2\n",
    "                agent.done = True\n",
    "        if sum(1 for agent in drop_agents if agent.done) == len(drop_agents) and sum(1 for agent in pick_agents if agent.done) == len(pick_agents):\n",
    "            self.done = True\n",
    "\n",
    "        return self.state_of_the_world(), self.done # So each time a step is taken the comeback is these three things.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def state_of_the_world(self): # This environment only has the spots of agents nd the package picked as its data.\n",
    "        return [a.start_position for a in self.agents], self.has_package\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders grid world.\n",
    "        \"\"\"\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.pick_up_position:\n",
    "                    print(\"P\", end=\" \")\n",
    "                elif (i, j) == self.delivery_position:\n",
    "                    print(\"D\", end=\" \")\n",
    "                elif (i, j) == self.agents[0].start_position:\n",
    "                    print(\"1\", end=\" \") #type 1 agent Pickup\n",
    "                elif (i, j) == self.agents[1].start_position:\n",
    "                    print('2', end=' ') #type 2 agent delivery\n",
    "                else:\n",
    "                    print(\".\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def reset(self):\n",
    "        self.initialize_agents()\n",
    "        self.package_picked = False\n",
    "        return self.state_of_the_world() #This works because all the method does is set all the world attributes to above.\n",
    "\n",
    "    ############################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pthGQl3ZKFuN"
   },
   "source": [
    "## Agent Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3kRMpX6bOaZ"
   },
   "source": [
    "### Define Deep Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "9PZPT_d6bOaZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "class DQN:\n",
    "\n",
    "    ############################################################################################################################\n",
    "    \"\"\"Deep Q-Network (DQN) agent.\"\"\"\n",
    "    def __init__(self, state_space, action_space=4, sampling=100, gamma=0.99, memory=None):\n",
    "        # implementation from skeleton\n",
    "        input_layer = state_space\n",
    "        layer_2 = 24\n",
    "        layer_3 = 24\n",
    "        output_layer = action_space\n",
    "        self.nn_initializer(input_layer, layer_2, layer_3, output_layer)\n",
    "        # Initialize short term memory\n",
    "        \n",
    "        #implementation kept from part 2\n",
    "        self.start_sampling = sampling # Experience sampling\n",
    "        self.gamma= gamma\n",
    "        #Short term memory, comparing the Q network information.\n",
    "        self.memory = memory if memory else ReplayBuffer(action_space, buffer_size=10000, batch_size=32) # User Defined above ______^\n",
    "        self.action_space=action_space\n",
    "############################################################################################################################\n",
    "    # The number of starting nodes: input, two hidden layers, and output\n",
    "    def nn_initializer (self, two, three, out, inn=5):\n",
    "        device = torch.device('cpu')\n",
    "        self.model_local = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, two), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(two, three), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(three, out)).to(device)\n",
    "\n",
    "        self.model_global = copy.deepcopy(self. model_local).to(device)\n",
    "        self.model_global.load_state_dict(self.model_local.state_dict())\n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model_local.parameters(), lr=0.001)\n",
    "\n",
    "############################################################################################################################\n",
    "    def get_q_vals(self, state): # The Q value list\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        return self.model_local(state)\n",
    "\n",
    "############################################################################################################################\n",
    "    def update_target(self):\n",
    "        self.model_global.load_state_dict(self.model_local.state_dict())\n",
    "\n",
    "############################################################################################################################\n",
    "    def act(self, state, epsilon=0.):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state using epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            state (torch.Tensor): Current state.\n",
    "            epsilon (float): Epsilon for epsilon-greedy action selection.\n",
    "        \"\"\"\n",
    "        print(state)\n",
    "        flattend_state_info = [c for sub in state[0] for c in sub]\n",
    "        add_picked = [int(state[1])]\n",
    "        flat_everything = np.array(flattend_state_info + add_picked).astype(np.float64)\n",
    "        print(flat_everything.shape)\n",
    "        state = torch.from_numpy(flat_everything)\n",
    "        state = state.float().unsqueeze(0).to(device) # here we take the state tensor and add another dimension at the start of this tensor\n",
    "        print(state.shape)\n",
    "        self.model_local.eval() # this sets the nn from torch into evaluation mode.\n",
    "        with torch.no_grad(): # Disable gradient tracking (local net doesn't learn from the calculation following:::)\n",
    "            action_values = self.model_local(state) # compute the value of taking each act --- then store it.\n",
    "        self.model_local.train() # return local q net into train mode (learn from acting in XYZ way)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() > epsilon: # zero and 1 random selection number comparison with epsilon\n",
    "            return np.argmax(action_values.cpu().data.numpy())  # should epsilon be smaller, this retrieves the calculated values from processing unit\n",
    "                                                                # puts it into a numpy array. picks the highest reward\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_space)) # randomly makes a choice for its next step.\n",
    "\n",
    "############################################################################################################################\n",
    "    def get_maxQ(self, state):\n",
    "        q_values = self.model(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "############################################################################################################################\n",
    "    def fit(self, env, n_episodes=5000, max_t=100,\n",
    "            rolling_epochs=200, target_score=None,\n",
    "            eps_start=1.0, eps_end=0.01, eps_decay=0.99):\n",
    "        \"\"\"\n",
    "        Train the agent using deep Q-learning.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            env: Environment to train in.\n",
    "            n_episodes (int): Maximum number of training episodes.\n",
    "            max_t (int): Maximum number of time steps per episode.\n",
    "            rolling_epochs (int): Number of episodes for calculating average score.\n",
    "            target_score (float): Target score for the environment.\n",
    "            eps_start (float): Starting value of epsilon.\n",
    "            eps_end (float): Minimum value of epsilon.\n",
    "            eps_decay (float): Decay factor for epsilon.\n",
    "        \"\"\"\n",
    "        scores = []  # List to store scores (rewards) from each episode\n",
    "        scores_window = deque(maxlen=rolling_epochs)  # Double ended que, once rolling_epochs is reached by one of the ends, the other end is bumped.\n",
    "        begin = eps_start # The local variable is what gives flexibility from the input param.\n",
    "        for epis in range(1, n_episodes + 1): # Simply for each episode...:\n",
    "            state = env.state_of_the_world() # Reset environment\n",
    "            print(type(state), state)\n",
    "            for _ in range(max_t): # For everything from 0 to the max allowed time\n",
    "                action = self.act(state, begin) # Calling the action function.\n",
    "                next_state, done = env.step(action) # Calling the step function from the \"grid\" getting reward values for taking steps, getting items etc..\n",
    "                self.step(state, action, next_state, done) # Take actual step based on the above line's values.\n",
    "                state = next_state # Updating the state variable\n",
    "                if done: # Done condition is package delivery.\n",
    "                    break\n",
    "\n",
    "            scores_window.append(score) # The last step's reward update is put into the double end queue.\n",
    "            scores.append(score) # Adds the score to the agent's personal record.\n",
    "            begin = max(eps_end, eps_decay * begin) # Reset the begin variable to the max of either the ending episode or decay times begin.\n",
    "\n",
    "            # Display training progress\n",
    "            print(f'\\rEpisode {epis}, Average Score: {np.mean(scores_window):.2f}', end=\"\")\n",
    "\n",
    "            if epis % rolling_epochs == 0:\n",
    "                print(f'\\rEpisode {epis}, Average Score: {np.mean(scores_window):.2f}')\n",
    "\n",
    "            if target_score and np.mean(scores_window) >= target_score:\n",
    "                torch.save(self.model_local.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "\n",
    "                    # Plot the scores\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        plt.plot(range(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.show()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def train_one_step(self, experiences):\n",
    "\n",
    "        targets_reply = [] #What the hell is this for? replay buff\n",
    "        print(\"expereinces\",experiences)\n",
    "        states, actions, q_rewards, next_states, dones = experiences #Next states and done are not used\n",
    "\n",
    "        state_batch = torch.cat([s.float() for s in states]) #Why does this have a state size of 224 and not 104\n",
    "        print(state_batch.size())\n",
    "        action_batch = torch.tensor(actions)\n",
    "        Q_local_expected = self.model_local(state_batch)\n",
    "\n",
    "\n",
    "        X = Q_local_expected.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "        Y = torch.tensor(q_rewards).float()\n",
    "        loss = self.loss_func(X, Y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_target(self.model_local, self.model_global)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory and learn if enough samples are available.\"\"\"\n",
    "\n",
    "        # Store experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done) #Each agent has its own memory attribute, holding the parameters inside. Now i gotta know what replay buffer is doing.\n",
    "\n",
    "        # Learn from a random subset of experiences if enough samples are available\n",
    "        if len(self.memory) > self.start_sampling: # if theres open memory randomly fill it with samples\n",
    "            experiences = self.memory.sample() # sampling the replay buffer obj from torch\n",
    "            self.train_one_step(experiences) # calls the learn method with above line executed and stored and the set future reward gamma discount rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQb-bdoMbOaa"
   },
   "source": [
    "### Agent Instantiation. Policy learning and Visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "P8UziM0uKFuO"
   },
   "outputs": [],
   "source": [
    "import torch as fire\n",
    "class MultiAgentHandler: # Connection to the Dairy Queen Network\n",
    "    def __init__(self, num_agents, state_space, action_space=4,**kwargs):\n",
    "        self.shared_memory = ReplayBuffer(action_space, buffer_size=10000, batch_size=64)\n",
    "        self.agents = [DQN(state_space, action_space,memory=self.shared_memory, **kwargs) for _ in range(num_agents)]\n",
    "\n",
    "    def train_agents(self, environment, n_episodes=1000):\n",
    "        for _ in range(1, n_episodes+1):\n",
    "            states = environment.reset() #start a fresh instance.\n",
    "            scores = [0]*len(self.agents) # each agent gets its score.\n",
    "\n",
    "            while True: #This runs through all the agents at their states and they move.\n",
    "                actions = [a.act(st) for a, st in zip(self.agents,states)]\n",
    "                next_state, rewards, done = environment.step(actions)\n",
    "\n",
    "                for agent, state, action, reward, next_state, done in zip(self.agents,states,actions,rewards,next_state,done):\n",
    "                    self.shared_memory.add(state,action, reward, next_state, done) #basically store a memory (remember it.)\n",
    "                    agent.shared_memory.step(state, action, reward, next_state, done) # Here we use the shared mem across erry body.\n",
    "\n",
    "                scores = [score + reward for score, reward in zip(scores,rewards)]\n",
    "                states = next_state\n",
    "\n",
    "                if any(done):\n",
    "                    break\n",
    "            # print scores / logging output\n",
    "            if _ % 100 == 0:\n",
    "                print(f\"Episode {_}, Average Score: {np.mean(scores):.2f}\")\n",
    "\n",
    "    def save(self):\n",
    "        for i, a in enumerate(self.agents):\n",
    "            fire.save(a.model_local.state_dict(), f'checkpoint_agent_{i}.pth') #This is where the agents are connected to their brains.\n",
    "\n",
    "    def load(self):\n",
    "        for i, a in enumerate(self.agents):\n",
    "            a.model_local.load_state_dict(fire.load(f'checkpoint_agent_{i}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j4d4gVQwbOab",
    "outputId": "9df96a13-24e2-4be7-9928-5828523329fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> ([(2, 1), (2, 0)], False)\n",
      "([(2, 1), (2, 0)], False)\n",
      "(5,)\n",
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'done'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m policy \u001b[38;5;241m=\u001b[39m DQN(state_space\u001b[38;5;241m=\u001b[39menvironment\u001b[38;5;241m.\u001b[39msize, action_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# Use the DQN class to govern agent object.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train agent object with GridWorld environment.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Training process for n_episodes or until desired score is reached.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# over the last 'rolling_epochs' (defaulted to 200 in the DQN class) reaches or exceeds 2.2.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36mDQN.fit\u001b[1;34m(self, env, n_episodes, max_t, rolling_epochs, target_score, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m    106\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(state, begin) \u001b[38;5;66;03m# Calling the action function.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m next_state, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# Calling the step function from the \"grid\" getting reward values for taking steps, getting items etc..\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Take actual step based on the above line's values.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state \u001b[38;5;66;03m# Updating the state variable\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# Done condition is package delivery.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'done'"
     ]
    }
   ],
   "source": [
    "# Initialize a 4x4 (n^2) grid world environment.\n",
    "environment = MultiAgentGridWorld(size=5)\n",
    "# Set agent's personal state size to the environment's state size.\n",
    "# The possible actions in this environment set to 4: up, down, left, right.\n",
    "\n",
    "policy = DQN(state_space=environment.size, action_space=4) # Use the DQN class to govern agent object.\n",
    "\n",
    "# Train agent object with GridWorld environment.\n",
    "# Training process for n_episodes or until desired score is reached.\n",
    "# over the last 'rolling_epochs' (defaulted to 200 in the DQN class) reaches or exceeds 2.2.\n",
    "policy.fit(env=environment, n_episodes=2500, target_score=2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1mC7tqSKFuP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqRU_2QbbOac"
   },
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8Ld8hlXbOad",
    "outputId": "42331627-d31e-4339-a2cc-f495f0d9a420"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoint.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jwick1/Desktop/School_Career/Monash/Past Semesters/Semester 2 2023/MAS/Assessment /p3/_p3sv1.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jwick1/Desktop/School_Career/Monash/Past%20Semesters/Semester%202%202023/MAS/Assessment%20/p3/_p3sv1.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load pretrained weights\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jwick1/Desktop/School_Career/Monash/Past%20Semesters/Semester%202%202023/MAS/Assessment%20/p3/_p3sv1.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m policy\u001b[39m.\u001b[39mmodel_local\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mcheckpoint.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint.pth'"
     ]
    }
   ],
   "source": [
    "# load pretrained weights\n",
    "policy.model_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZ2tDUvZbOae",
    "outputId": "3b0a633f-6dfc-49e4-b13b-acee3138d8e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . \n",
      ". . . . \n",
      ". F . . \n",
      ". R . . \n"
     ]
    }
   ],
   "source": [
    "# Visualise 5 episodes using trained agent\n",
    "\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    for j in range(20):\n",
    "        action = policy.act(state)\n",
    "        state, reward, done = env.step(action)\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(1)\n",
    "        if done:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
