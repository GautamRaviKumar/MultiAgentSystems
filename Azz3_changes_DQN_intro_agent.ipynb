{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGag52BXbOaM"
   },
   "source": [
    "# Robot Grid World Assessment part 2\n",
    "**Written by**  \n",
    "Egor Danilov (33411115),  \n",
    "Yash Balchandani(33279950),  \n",
    "Gautam Ravi Kumar(33197970),  \n",
    "Jacob Wicklund (31265936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD3VCWYYbOaS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, namedtuple, deque\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torch import tensor, optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "import numpy.random as npr\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlNwyGBFbOaU"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A class representing a grid world environment for a robot to navigate and complete tasks.\n",
    "\n",
    "    Attributes:\n",
    "        state_tuple: namedtuple class for representing the state of the environment.\n",
    "        n: Size of the grid world.\n",
    "        action_space: List of possible actions (0: up, 1: down, 2: left, 3: right).\n",
    "        action_map: Dictionary mapping actions to their corresponding (row, column) changes.\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, n: int = 5): Constructor to initialize the grid world.\n",
    "        _next_robot_position(self, action): Calculates the next position of the robot based on the action taken.\n",
    "        step(self, action): Performs an action in the environment and returns the new state, reward, and done flag.\n",
    "        reward(self, action): Computes the reward based on the action and the current state.\n",
    "        reset(self): Resets the environment to create a new grid world configuration.\n",
    "        render(self): Renders the current state of the grid world.\n",
    "    \"\"\"\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, n: int=5, agents_each: int=2):\n",
    "        \"\"\"\n",
    "        Initializes a GridWorld instance.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            n (int): Size of the grid world (default is 5).\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = n\n",
    "        self.agents_each = agents_each\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.pickup_reward = 25\n",
    "        self.dropoff_reward = 25\n",
    "        self.handover_reward = 50\n",
    "        self.action_map = {\n",
    "            0: (tensor(-1), tensor(0)),  # up\n",
    "            1: (tensor(1), tensor(0)),   # down\n",
    "            2: (tensor(0), tensor(-1)),  # left\n",
    "            3: (tensor(0), tensor(1))    # right\n",
    "        }\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def _next_robot_position(self, actions):\n",
    "        next_robot_locs = []\n",
    "        for i in range(self.agents_each * 2):\n",
    "            next_robot_loc = (self.state[i*2] + self.action_map[actions[i]][0], self.state[i*2 + 1] + self.action_map[actions[i]][1])\n",
    "            next_robot_locs.append(next_robot_loc)\n",
    "        return next_robot_locs\n",
    "\n",
    "    def step(self, action_1, action_2, action_3, action_4):\n",
    "        done = False\n",
    "        picked = self.state[12:]\n",
    "        flag = False\n",
    "\n",
    "        # Convert tensor values to Python integers for calculations\n",
    "        state_values = self.state.numpy()\n",
    "        next_robot_locs = self._next_robot_position([action_1, action_2, action_3, action_4])\n",
    "        for i in range(self.agents_each * 2):\n",
    "            next_robot_x, next_robot_y = next_robot_locs[i]\n",
    "            # Ensure the robot stays within bounds\n",
    "            if 0 <= next_robot_x <= self.n - 1 and 0 <= next_robot_y <= self.n - 1:\n",
    "                self.steps_taken[i] += 1\n",
    "                state_values[i * 2] = next_robot_x\n",
    "                state_values[i * 2 + 1] = next_robot_y\n",
    "            # Check if a \"P\" agent is at the same location as a \"D\" agent and the \"P\" agent is carrying a load\n",
    "        for i in range(self.agents_each):\n",
    "            if state_values[i * 2] == state_values[8] and state_values[i * 2 + 1] == state_values[9] and not picked[i]:\n",
    "                picked[i] = True\n",
    "                self.rewards[i] += self.reward(self.pickup_reward, self.steps_taken[i])\n",
    "            else:\n",
    "                self.rewards[i] = 0\n",
    "            for j in range(self.agents_each, self.agents_each * 2):\n",
    "                if state_values[j * 2] == state_values[i * 2] and state_values[j * 2 + 1] == state_values[i * 2 + 1] and picked[i] and not picked[j]:\n",
    "                    picked[i] = False\n",
    "                    self.rewards[i] += self.reward(self.handover_reward, self.steps_taken[i],type=1)\n",
    "                    self.steps_taken[i] = 0\n",
    "                    picked[j] = True\n",
    "                    self.rewards[j] += self.reward(self.handover_reward, self.steps_taken[j],type=1)\n",
    "                    self.steps_taken[j] = 0\n",
    "                    flag = True\n",
    "            # Check if a \"D\" agent is at the final location and has a load to deliver\n",
    "        for i in range(self.agents_each, self.agents_each * 2):\n",
    "            if state_values[i * 2] == state_values[10] and state_values[i * 2 + 1] == state_values[11] and picked[i]:\n",
    "                picked[i] = False\n",
    "                self.rewards[i] += self.reward(self.dropoff_reward, self.steps_taken[i])\n",
    "                self.steps_taken[i] = 0\n",
    "                flag = True\n",
    "                #done = True\n",
    "            if flag == False:\n",
    "                self.rewards[i] = 0\n",
    "        flag = False\n",
    "\n",
    "\n",
    "        # Convert state values back to a tensor\n",
    "        self.state = torch.tensor(state_values)\n",
    "\n",
    "        #reward = self.reward(action)\n",
    "\n",
    "        return self.state, self.rewards, done\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def render(self):\n",
    "        Robot_loc_x = self.state[0:4:2]  # Extract x-coordinates of the first set of agents\n",
    "        Robot_loc_y = self.state[1:4:2]  # Extract y-coordinates of the first set of agents\n",
    "        Robot_loc_x_2 = self.state[4:8:2]  # Extract x-coordinates of the second set of agents\n",
    "        Robot_loc_y_2 = self.state[5:8:2]  # Extract y-coordinates of the second set of agents\n",
    "        Load_loc_x = int(self.state[8])\n",
    "        Load_loc_y = int(self.state[9])\n",
    "        Final_loc_x = int(self.state[10])\n",
    "        Final_loc_y = int(self.state[11])\n",
    "        picked = self.state[12:]\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                agent_here = False\n",
    "                for k in range(len(Robot_loc_x)):\n",
    "                    if (i, j) == (Robot_loc_x[k], Robot_loc_y[k]):\n",
    "                        agent_here = True\n",
    "                        print(\"P\", end=\" \")\n",
    "                        break\n",
    "                for k in range(len(Robot_loc_x_2)):\n",
    "                    if (i, j) == (Robot_loc_x_2[k], Robot_loc_y_2[k]):\n",
    "                        agent_here = True\n",
    "                        print(\"D\", end=\" \")\n",
    "                        break\n",
    "                if not agent_here:\n",
    "                    if (i, j) == (Load_loc_x, Load_loc_y) and not picked[k]:\n",
    "                        print(\"L\", end=\" \")\n",
    "                    elif (i, j) == (Final_loc_x, Final_loc_y):\n",
    "                        print(\"F\", end=\" \")\n",
    "                    else:\n",
    "                        print(\".\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def reset(self):\n",
    "        n = 5  # Replace with your desired value of n\n",
    "        agents_each = 2  # Replace with your desired number of agents for each action\n",
    "\n",
    "        # Generate unique random indexes within the range from 1 to n * n - 1\n",
    "        indexes = np.random.choice(range(1, n * n - 1), agents_each * 2, replace=False)\n",
    "        self.steps_taken = [0] * (agents_each * 2)\n",
    "        self.rewards = [0] * (agents_each * 2)\n",
    "        Robot_loc_x = [0] * (agents_each * 2)  # Pre-allocate the list with zeros\n",
    "        Robot_loc_y = [0] * (agents_each * 2)  # Pre-allocate the list with zeros\n",
    "        picked = [False] * (agents_each * 2)  # Pre-allocate the list with False\n",
    "\n",
    "        Load_loc_x = 0\n",
    "        Load_loc_y = 0\n",
    "        Final_loc_x = n - 1\n",
    "        Final_loc_y = n - 1\n",
    "\n",
    "        for i in range(agents_each * 2):\n",
    "            x = indexes[i] % n\n",
    "            y = indexes[i] // n\n",
    "            Robot_loc_x[i] = x\n",
    "            Robot_loc_y[i] = y\n",
    "\n",
    "        # Create a tensor to represent the state\n",
    "        self.state = tensor([Robot_loc_x[0], Robot_loc_y[0],Robot_loc_x[1], Robot_loc_y[1],Robot_loc_x[2], Robot_loc_y[2],Robot_loc_x[3], Robot_loc_y[3], Load_loc_x, Load_loc_y, Final_loc_x, Final_loc_y, picked[0],picked[1],picked[2],picked[3]])\n",
    "\n",
    "        # print(\"State tensor:\")\n",
    "        # print(self.state)\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def reward(self, reward_amount,steps_taken,type=0):\n",
    "        \"\"\"\n",
    "        Computes the immediate reward based on the actions and the current state.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            actions (list): List of actions taken by the robots.\n",
    "\n",
    "        Returns:\n",
    "            float: Reward value.\n",
    "        \"\"\"\n",
    "        if type == 1:\n",
    "            return reward_amount - steps_taken**2\n",
    "        else :\n",
    "            return reward_amount\n",
    "\n",
    "\n",
    "    ############################################################################################################################\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return 14\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAMLQSPwD2OO",
    "outputId": "882e3473-6caa-435b-8c39-16f10eef87ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 2, 4, 0, 3, 0, 4, 0, 0, 4, 4, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgESdb93TCnW",
    "outputId": "865c960a-3050-4641-d76a-b844ab97625b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L . . D D \n",
      ". . . . . \n",
      ". . . . P \n",
      ". . . . . \n",
      ". . . P F \n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7bvsLEPTmTC",
    "outputId": "6622cab2-3a71-4071-b71b-8444cc9b822d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 1, 4, 0, 2, 0, 4, 0, 0, 4, 4, 0, 0, 0, 0])\n",
      "[0, 0, 0, 0]\n",
      "L . D . D \n",
      ". . . . P \n",
      ". . . . . \n",
      ". . . P . \n",
      ". . . . F \n"
     ]
    }
   ],
   "source": [
    "next_state, rewards, done = env.step(0, 0, 2, 0)\n",
    "print(next_state)\n",
    "print(rewards)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gJzU408bOaX"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-Network model used for Deep Q-Learning.\"\"\"\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=32):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Network.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            state_size (int): Dimension of each state (input size).\n",
    "            action_size (int): Dimension of each action (output size).\n",
    "            fc1_units (int): Number of nodes in the first hidden layer.\n",
    "            fc2_units (int): Number of nodes in the second hidden layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Build the neural network that maps state -> action values.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            state (Tensor): The input state for which to compute the action values.\n",
    "\n",
    "        Returns:\n",
    "            x (Tensor): The action values for the given state.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEpqV2ZvbOaY"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize a ReplayBuffer object.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            action_size (int): Dimensionality of each action.\n",
    "            buffer_size (int): Maximum number of experiences to store in the buffer.\n",
    "            batch_size (int): Number of experiences to sample during training.\n",
    "        \"\"\"\n",
    "        self.action_size = action_size # Maximum possible ways each action can be done.\n",
    "\n",
    "        self.memory = deque(maxlen=buffer_size) #double ended queue for a short term memory of agent.\n",
    "\n",
    "        self.batch_size = batch_size # Resampling of the past experience what is past experience? Named Tuple from part 1.\n",
    "\n",
    "        # (Tagged) named tuple to represent a single experience in the replay memory\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory) # That is the dbl qu length.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to memory.\n",
    "        \"\"\"\n",
    "        exp = self.experience(state, action, reward, next_state, done) # Experience tuple. Defined above.\n",
    "        # Pass in the experience to the short term memory queue.\n",
    "        self.memory.append(exp) # Passes the list of attributes into the memory dbl queue.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def sample(self): #Segway into the deep q network. This is the sampler method.\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences from memory for training.\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size) # Sample all the past experiences to learn the most important qualities so we can delete the experience data.\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device) #retrieve the states from experience stored in double qu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device) # retrieve the actions\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device) # the rewards\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device) # possible next moves, choosing the probabilistic best reward. retrieved from the DQN calculation\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device) # task completed flag. IE shut down.\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones) # send this info back to caller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3kRMpX6bOaZ"
   },
   "source": [
    "### Define Deep Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMsIXtQ0_oQj"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"agent\"\"\"\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, state_size, action_size,\n",
    "                buffer_size: int=100000, batch_size: int=32,\n",
    "                learning_rate: float=3e-4, gamma: float=0.99,\n",
    "                start_sampling: int=100):\n",
    "\n",
    "        self.state_size = state_size # number of possible states.\n",
    "        self.action_size = action_size # how many possible actions.\n",
    "        self.start_sampling = start_sampling # \"experiences of the agent object\".\n",
    "        self.buffer_size = buffer_size # replay buffer.\n",
    "        self.batch_size = batch_size # Size of the train batch...?\n",
    "        self.learning_rate = learning_rate # Network optimizing parameter.\n",
    "        self.gamma = gamma # Discounting the rewards where as the most immediate one is carrying the most weight.\n",
    "\n",
    "        # Initialize two Q-Networks: local and target\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device) # primary train \"expected value\" network\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device) #secondary \"expected value\" network for stabilizing the dynamic behavior of the primary network.\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate) # This is a basic initializer for optimization of the NN.\n",
    "\n",
    "        # Initialize short term memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size) # User Defined above ______^\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Update Q-network parameters using a batch of experience tuples.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            experiences (Tuple[torch.Tensor]): Tuple of (s, a, r, s', done).\n",
    "            gamma (float): Discount factor.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences # Split the tensor into respective variables the named tuple of length 5\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) # Q (expected return) from the target network.\n",
    "        # Detach the current matrix from a tensor, so its not included in updating.\n",
    "        # calculate the maximum of the first 0th element which is the states, carrying their respective rewards from the map\n",
    "        # unsqueeze (grow)_ is just the tensor reshape to add another dimension along the position 1 element\n",
    "        # Giving the next best target for a future calculation.v\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones)) # Best decision given the current rules and move limitation - initialized by the policy.\n",
    "        # Also taking the Q-rewards out of the detached tensor and adding it into the second half of the bellmen equation\n",
    "        # Get expected Q values from local network\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions) # Expected 'rewards' (equal to Q-rewards) out of all next possible states.\n",
    "\n",
    "        # Optimize\n",
    "        loss = F.mse_loss(Q_expected, Q_targets) # Neural network functions, computing the temporal difference error.\n",
    "        # holding the knowledge of the best rewards for an mse loss between expectation and target. Minimizing this difference is the DQ goal.\n",
    "\n",
    "        self.optimizer.zero_grad() # This is like a memory reset for the gradients, so previous runs aren't stored in new runs.\n",
    "        loss.backward() # Do one backward pass through gradients to see the optimum path to best return.\n",
    "        self.optimizer.step() # update \"weights and biases\" through the game board network. According to optimizer set point, Adam in this case.\n",
    "\n",
    "        # Update target network\n",
    "        self.update(self.qnetwork_local, self.qnetwork_target) # This aligns the local network (possible moves) with the target network \"game board\".\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory and learn if enough samples are available.\"\"\"\n",
    "\n",
    "        # Store experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done) #Each agent has its own memory attribute, holding the parameters inside. Now i gotta know what replay buffer is doing.\n",
    "\n",
    "        # Learn from a random subset of experiences if enough samples are available\n",
    "        if len(self.memory) > self.start_sampling: # if theres open memory randomly fill it with samples\n",
    "            experiences = self.memory.sample() # sampling the replay buffer obj from torch\n",
    "            self.learn(experiences, self.gamma) # calls the learn method with above line executed and stored and the set future reward gamma discount rate.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state using epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            state (torch.Tensor): Current state.\n",
    "            eps (float): Epsilon for epsilon-greedy action selection.\n",
    "        \"\"\"\n",
    "        state = state.float().unsqueeze(0).to(device) # here we take the state tensor and add another dimension at the start of this tensor\n",
    "        self.qnetwork_local.eval() # this sets the nn from torch into evaluation mode.\n",
    "        with torch.no_grad(): # Disable gradient tracking (local net doesn't learn from the calculation following:::)\n",
    "            action_values = self.qnetwork_local(state) # compute the value of taking each act --- then store it.\n",
    "        self.qnetwork_local.train() # return local q net into train mode (learn from acting in XYZ way)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() > eps: # zero and 1 random selection number comparison with epsilon\n",
    "            return np.argmax(action_values.cpu().data.numpy())  # should epsilon be smaller, this retrieves the calculated values from processing unit\n",
    "                                                                # puts it into a numpy array. picks the highest reward\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size)) # randomly makes a choice for its next step.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def update(self, local_model, target_model):\n",
    "        \"\"\"Copy weights from local model to target model.\"\"\"\n",
    "        target_model.load_state_dict(local_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDZMXPoBANaJ"
   },
   "outputs": [],
   "source": [
    "def swap_pick_up_pairs(tensor):\n",
    "    # Ensure the tensor has at least 14 elements\n",
    "    if tensor.numel() < 14:\n",
    "        raise ValueError(\"Tensor must have at least 14 elements\")\n",
    "\n",
    "    # Create a copy of the tensor\n",
    "    swapped_tensor = tensor.clone()\n",
    "\n",
    "    # Swap the pairs in the copied tensor\n",
    "    swapped_tensor[0:2], swapped_tensor[2:4] = tensor[2:4], tensor[0:2]\n",
    "\n",
    "    # Swap the elements with indexes 12 and 13\n",
    "    swapped_tensor[12], swapped_tensor[13] = tensor[13], tensor[12]\n",
    "\n",
    "    return swapped_tensor\n",
    "\n",
    "# tensor = torch.tensor([2, 2, 2, 4, 3, 1, 4, 0, 0, 0, 4, 4, 0, 1, 0, 0])\n",
    "# print(swap_pick_up_pairs(tensor))  # The changed tensor\n",
    "# print(tensor)                      # The original tensor, unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNCWmLGkBwkY"
   },
   "outputs": [],
   "source": [
    "def swap_delivery_pairs(tensor):\n",
    "    # Ensure the tensor has at least 16 elements\n",
    "    if tensor.numel() < 16:\n",
    "        raise ValueError(\"Tensor must have at least 16 elements\")\n",
    "\n",
    "    # Create a copy of the tensor\n",
    "    swapped_tensor = tensor.clone()\n",
    "\n",
    "    # Swap the pairs in the copied tensor\n",
    "    swapped_tensor[4:6], swapped_tensor[6:8] = tensor[6:8], tensor[4:6]\n",
    "\n",
    "    # Swap the elements with indexes 14 and 15\n",
    "    swapped_tensor[14], swapped_tensor[15] = tensor[15], tensor[14]\n",
    "\n",
    "    return swapped_tensor\n",
    "\n",
    "# tensor = torch.tensor([2, 2, 2, 4, 3, 1, 4, 0, 0, 0, 4, 4, 0, 0, 0, 1])\n",
    "# print(swap_delivery_pairs(tensor))  # The changed tensor\n",
    "# print(tensor)                      # The original tensor, unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diTRgI2N_oQl",
    "outputId": "e72f3f9a-d6df-405a-c2c4-c92c90025f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Average Score: -3228.72\n",
      "Episode 400, Average Score: -3273.27\n",
      "Episode 600, Average Score: -2256.58\n",
      "Episode 800, Average Score: -2139.70\n",
      "Episode 830, Average Score: -1536.19"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the agent using deep Q-learning.\n",
    "\n",
    "Parameters:\n",
    "===========\n",
    "    env: Environment to train in.\n",
    "    n_episodes (int): Maximum number of training episodes.\n",
    "    max_t (int): Maximum number of time steps per episode.\n",
    "    rolling_epochs (int): Number of episodes for calculating average score.\n",
    "    target_score (float): Target score for the environment.\n",
    "    eps_start (float): Starting value of epsilon.\n",
    "    eps_end (float): Minimum value of epsilon.\n",
    "    eps_decay (float): Decay factor for epsilon.\n",
    "\"\"\"\n",
    "\n",
    "n_episodes = 1500\n",
    "max_t: int=200\n",
    "rolling_epochs=200\n",
    "target_score=None\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.99\n",
    "\n",
    "env = GridWorld()\n",
    "\n",
    "pick_up_agent = Agent(state_size=16, action_size=4)\n",
    "delivery_agent = Agent(state_size=16, action_size=4)\n",
    "\n",
    "scores = []  # List to store scores (rewards) from each episode\n",
    "scores_window = deque(maxlen=rolling_epochs)  # Double ended que, once rolling_epochs is reached by one of the ends, the other end is bumped.\n",
    "begin = eps_start # The local variable is what gives flexibility from the input param.\n",
    "for epis in range(1, n_episodes + 1): # Simply for each episode...:\n",
    "    state = env.reset() # Reset environment\n",
    "    score = 0 # Agent's reward score.\n",
    "    for _ in range(max_t): # For everything from 0 to the max allowed time\n",
    "        action_1 = pick_up_agent.act(state, begin) # Calling the action function.\n",
    "        swaped_pick_up_state = swap_pick_up_pairs(state)\n",
    "        action_2 = pick_up_agent.act(swaped_pick_up_state, begin) # Calling the action function.\n",
    "        # new_mapping = {0: (0, 0), 1:(0, 1)}\n",
    "\n",
    "        action_3 = delivery_agent.act(state, begin) # Calling the action function.\n",
    "        swaped_delivery_state = swap_delivery_pairs(state)\n",
    "        action_4 = delivery_agent.act(swaped_delivery_state, begin) # Calling the action function.\n",
    "\n",
    "        next_state, rewards, done = env.step(action_1, action_2, action_3, action_4) # Calling the step function from the \"grid\" getting reward values for taking steps, getting items etc..\n",
    "        pick_up_agent.step(state, action_1, rewards[0], next_state, done) # Take actual step based on the above line's values.\n",
    "        pick_up_agent.step(state, action_2, rewards[1], next_state, done) # Take actual step based on the above line's values.\n",
    "\n",
    "        delivery_agent.step(state, action_3, rewards[2], next_state, done) # Take actual step based on the above line's values.\n",
    "        delivery_agent.step(state, action_4, rewards[3], next_state, done) # Take actual step based on the above line's values.\n",
    "\n",
    "        state = next_state # Updating the state variable\n",
    "        score += np.mean(rewards) # Add the step consequence.\n",
    "        if done: # Done condition is package delivery.\n",
    "            break\n",
    "\n",
    "    scores_window.append(score) # The last step's reward update is put into the double end queue.\n",
    "    scores.append(score) # Adds the score to the agent's personal record.\n",
    "    begin = max(eps_end, eps_decay * begin) # Reset the begin variable to the max of either the ending episode or decay times begin.\n",
    "\n",
    "    # Display training progress\n",
    "    print(f'\\rEpisode {epis}, Average Score: {np.mean(scores_window):.2f}', end=\"\")\n",
    "\n",
    "    if epis % rolling_epochs == 0:\n",
    "        print(f'\\rEpisode {epis}, Average Score: {np.mean(scores_window):.2f}')\n",
    "\n",
    "    # if target_score and np.mean(scores_window) >= target_score:\n",
    "    #     torch.save(self.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    #     break\n",
    "\n",
    "            # Plot the scores\n",
    "fig = plt.figure(figsize=(6,3))\n",
    "plt.plot(range(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQb-bdoMbOaa"
   },
   "source": [
    "### Agent Instantiation. Policy learning and Visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqRU_2QbbOac"
   },
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8Ld8hlXbOad"
   },
   "outputs": [],
   "source": [
    "# load pretrained weights\n",
    "policy.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "BZ2tDUvZbOae",
    "outputId": "338badbc-5c71-431c-ce0f-195d14dcb2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L . . D . \n",
      ". D . . . \n",
      ". . . . . \n",
      ". . . . . \n",
      "P . P . F \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-be90f25b3ef8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Visualise 5 episodes using trained agent\n",
    "\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    for j in range(10):\n",
    "        action_1 = pick_up_agent.act(state, begin) # Calling the action function.\n",
    "        swaped_pick_up_state = swap_pick_up_pairs(state)\n",
    "        action_2 = pick_up_agent.act(swaped_pick_up_state, begin) # Calling the action function.\n",
    "\n",
    "        action_3 = delivery_agent.act(state, begin) # Calling the action function.\n",
    "        swaped_delivery_state = swap_delivery_pairs(state)\n",
    "        action_4 = delivery_agent.act(swaped_delivery_state, begin) # Calling the action function.\n",
    "\n",
    "        next_state, rewards, done = env.step(action_1, action_2, action_3, action_4)\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(1)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waBZHKgnj8FQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
