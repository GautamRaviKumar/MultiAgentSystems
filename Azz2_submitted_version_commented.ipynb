{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGag52BXbOaM"
   },
   "source": [
    "# Robot Grid World Assessment part 2\n",
    "**Written by**  \n",
    "Egor Danilov (33411115),  \n",
    "Yash Balchandani(33279950),  \n",
    "Gautam Ravi Kumar(33197970),  \n",
    "Jacob Wicklund (31265936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XD3VCWYYbOaS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, namedtuple, deque\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torch import tensor, optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GlNwyGBFbOaU"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A class representing a grid world environment for a robot to navigate and complete tasks.\n",
    "\n",
    "    Attributes:\n",
    "        state_tuple: namedtuple class for representing the state of the environment.\n",
    "        n: Size of the grid world.\n",
    "        action_space: List of possible actions (0: up, 1: down, 2: left, 3: right).\n",
    "        action_map: Dictionary mapping actions to their corresponding (row, column) changes.\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, n: int = 5): Constructor to initialize the grid world.\n",
    "        _next_robot_position(self, action): Calculates the next position of the robot based on the action taken.\n",
    "        step(self, action): Performs an action in the environment and returns the new state, reward, and done flag.\n",
    "        reward(self, action): Computes the reward based on the action and the current state.\n",
    "        reset(self): Resets the environment to create a new grid world configuration.\n",
    "        render(self): Renders the current state of the grid world.\n",
    "    \"\"\"\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, n: int=5):\n",
    "        \"\"\"\n",
    "        Initializes a GridWorld instance.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            n (int): Size of the grid world (default is 5).\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.action_map_ = {\n",
    "            0: -n,  # up\n",
    "            1: n,   # down\n",
    "            2: -1,  # left\n",
    "            3: 1    # right\n",
    "        }\n",
    "    ############################################################################################################################\n",
    "    def _next_robot_position(self, action):\n",
    "        \"\"\"\n",
    "        Calculates the next position of the robot based on the action taken.\n",
    "        If the robot tries to go out of bounds it is returned back.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            action (int): Action taken by the robot.\n",
    "\n",
    "        Returns:\n",
    "            tuple: New position of the robot.\n",
    "        \"\"\"\n",
    "        # Calculate the next robot location based on the action\n",
    "        next_robot_loc = (self.state[0] + self.action_map_[action][0],\n",
    "                        self.state[1] + self.action_map_[action][1])\n",
    "\n",
    "        # Ensure the robot stays within bounds\n",
    "        if (0 <= next_robot_loc[0] <= self.n - 1 and\n",
    "            0 <= next_robot_loc[1] <= self.n - 1):\n",
    "            return next_robot_loc\n",
    "\n",
    "        return self.state[0], self.state[1]\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action in the environment.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            action (int): Action taken by the robot.\n",
    "\n",
    "        Returns:\n",
    "            tuple: New state, immediate reward, and done flag.\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        picked = self.state[6]\n",
    "        next_robot_loc = self._next_robot_position(action)\n",
    "\n",
    "        # Check if robot has picked up the load\n",
    "        if next_robot_loc == (self.state[2], self.state[3]):\n",
    "            picked = True\n",
    "\n",
    "        # Check if robot reached the destination with the load\n",
    "        if next_robot_loc == (self.state[4], self.state[5]) and picked:\n",
    "            done = True\n",
    "\n",
    "        reward = self.reward(action)\n",
    "        self.state = tensor([next_robot_loc[0], next_robot_loc[1],\n",
    "                            self.state[2], self.state[3],\n",
    "                            self.state[4], self.state[5],\n",
    "                            picked])\n",
    "\n",
    "        return self.state, reward, done\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the current state of the grid world.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        Robot_loc_x, Robot_loc_y, Load_loc_x, Load_loc_y, Final_loc_x, Final_loc_y, picked = self.state\n",
    "\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                if (i, j) == (Robot_loc_x, Robot_loc_y):\n",
    "                    print(\"R\", end=\" \")\n",
    "                elif (i, j) == (Load_loc_x, Load_loc_y) and not picked:\n",
    "                    print(\"L\", end=\" \")\n",
    "                elif (i, j) == (Final_loc_x, Final_loc_y):\n",
    "                    print(\"F\", end=\" \")\n",
    "                else:\n",
    "                    print(\".\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment. Randomly creates a new grid world configuration.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        r, l, f = np.random.choice(self.n * self.n, size=3, replace=False)\n",
    "        Robot_loc_x, Robot_loc_y = r % self.n, r // self.n\n",
    "        Load_loc_x, Load_loc_y   = l % self.n, l // self.n\n",
    "        Final_loc_x, Final_loc_y = f % self.n, f // self.n\n",
    "        picked = False\n",
    "        locations = np.array([r,l,f]).reshape(-1)\n",
    "        self.state = np.eye(self.n * self.n)[locations]\n",
    "        self.state = np.append(self.state,picked)\n",
    "        self.state = tensor([Robot_loc_x, Robot_loc_y,Load_loc_x, Load_loc_y,Final_loc_x, Final_loc_y,picked])\n",
    "        return self.state\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def reward(self, action):\n",
    "        \"\"\"\n",
    "        Computes the immediate reward based on the action and the current state.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            action (int): Action taken by the robot.\n",
    "\n",
    "        Returns:\n",
    "            float: Reward value.\n",
    "        \"\"\"\n",
    "        next_robot_loc = self._next_robot_position(action)\n",
    "\n",
    "        # Reward for picking up the load\n",
    "        if next_robot_loc == (self.state[2], self.state[3]) and not self.state[6]:\n",
    "            return tensor(1.0)\n",
    "\n",
    "        # Reward for delivering the load to the final location\n",
    "        if next_robot_loc == (self.state[4], self.state[5]) and self.state[6]:\n",
    "            return tensor(2.0)\n",
    "\n",
    "        # Penalty for every step that is not the final location and not the load location\n",
    "        return tensor(-0.2)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return 49\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8gJzU408bOaX"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-Network model used for Deep Q-Learning.\"\"\"\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=32):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Network.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            state_size (int): Dimension of each state (input size).\n",
    "            action_size (int): Dimension of each action (output size).\n",
    "            fc1_units (int): Number of nodes in the first hidden layer.\n",
    "            fc2_units (int): Number of nodes in the second hidden layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Build the neural network that maps state -> action values.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            state (Tensor): The input state for which to compute the action values.\n",
    "\n",
    "        Returns:\n",
    "            x (Tensor): The action values for the given state.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yEpqV2ZvbOaY"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize a ReplayBuffer object.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            action_size (int): Dimensionality of each action.\n",
    "            buffer_size (int): Maximum number of experiences to store in the buffer.\n",
    "            batch_size (int): Number of experiences to sample during training.\n",
    "        \"\"\"\n",
    "        self.action_size = action_size # Maximum possible ways each action can be done.\n",
    "\n",
    "        self.memory = deque(maxlen=buffer_size) #double ended queue for a short term memory of agent.\n",
    "\n",
    "        self.batch_size = batch_size # Resampling of the past experience what is past experience? Named Tuple from part 1.\n",
    "\n",
    "        # (Tagged) named tuple to represent a single experience in the replay memory\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory) # That is the dbl qu length.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to memory.\n",
    "        \"\"\"\n",
    "        exp = self.experience(state, action, reward, next_state, done) # Experience tuple. Defined above.\n",
    "        # Pass in the experience to the short term memory queue.\n",
    "        self.memory.append(exp) # Passes the list of attributes into the memory dbl queue.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def sample(self): #Segway into the deep q network. This is the sampler method.\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences from memory for training.\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size) # Sample all the past experiences to learn the most important qualities so we can delete the experience data.\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device) #retrieve the states from experience stored in double qu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device) # retrieve the actions\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device) # the rewards\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device) # possible next moves, choosing the probabilistic best reward. retrieved from the DQN calculation\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device) # task completed flag. IE shut down.\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones) # send this info back to caller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3kRMpX6bOaZ"
   },
   "source": [
    "### Define Deep Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9PZPT_d6bOaZ"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \"\"\"Deep Q-Network (DQN) agent.\"\"\"\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, state_size, action_size,\n",
    "                buffer_size: int=100000, batch_size: int=32,\n",
    "                learning_rate: float=3e-4, gamma: float=0.99,\n",
    "                start_sampling: int=100):\n",
    "\n",
    "        self.state_size = state_size # number of possible states.\n",
    "        self.action_size = action_size # how many possible actions.\n",
    "        self.start_sampling = start_sampling # \"experiences of the agent object\".\n",
    "        self.buffer_size = buffer_size # replay buffer.\n",
    "        self.batch_size = batch_size # Size of the train batch...?\n",
    "        self.learning_rate = learning_rate # Network optimizing parameter.\n",
    "        self.gamma = gamma # Discounting the rewards where as the most immediate one is carrying the most weight.\n",
    "\n",
    "        # Initialize two Q-Networks: local and target\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device) # primary train \"expected value\" network\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device) #secondary \"expected value\" network for stabilizing the dynamic behavior of the primary network.\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate) # This is a basic initializer for optimization of the NN.\n",
    "\n",
    "        # Initialize short term memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size) # User Defined above ______^\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state using epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            state (torch.Tensor): Current state.\n",
    "            eps (float): Epsilon for epsilon-greedy action selection.\n",
    "        \"\"\"\n",
    "        #state = state.float().unsqueeze(0).to(device) # here we take the state tensor and add another dimension at the start of this tensor\n",
    "        self.qnetwork_local.eval() # this sets the nn from torch into evaluation mode.\n",
    "        with torch.no_grad(): # Disable gradient tracking (local net doesn't learn from the calculation following:::)\n",
    "            action_values = self.qnetwork_local(state) # compute the value of taking each act --- then store it.\n",
    "        self.qnetwork_local.train() # return local q net into train mode (learn from acting in XYZ way)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() > eps: # zero and 1 random selection number comparison with epsilon\n",
    "            return np.argmax(action_values.cpu().data.numpy())  # should epsilon be smaller, this retrieves the calculated values from processing unit\n",
    "                                                                # puts it into a numpy array. picks the highest reward\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size)) # randomly makes a choice for its next step.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def fit(self, env, n_episodes: int=5000, max_t: int=100,\n",
    "            rolling_epochs: int=200, target_score=None,\n",
    "            eps_start=1.0, eps_end=0.01, eps_decay=0.99):\n",
    "        \"\"\"\n",
    "        Train the agent using deep Q-learning.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            env: Environment to train in.\n",
    "            n_episodes (int): Maximum number of training episodes.\n",
    "            max_t (int): Maximum number of time steps per episode.\n",
    "            rolling_epochs (int): Number of episodes for calculating average score.\n",
    "            target_score (float): Target score for the environment.\n",
    "            eps_start (float): Starting value of epsilon.\n",
    "            eps_end (float): Minimum value of epsilon.\n",
    "            eps_decay (float): Decay factor for epsilon.\n",
    "        \"\"\"\n",
    "        scores = []  # List to store scores (rewards) from each episode\n",
    "        scores_window = deque(maxlen=rolling_epochs)  # Double ended que, once rolling_epochs is reached by one of the ends, the other end is bumped.\n",
    "        begin = eps_start # The local variable is what gives flexibility from the input param.\n",
    "        for epis in range(1, n_episodes + 1): # Simply for each episode...:\n",
    "            state = env.reset() # Reset environment\n",
    "            score = 0 # Agent's reward score.\n",
    "            for _ in range(max_t): # For everything from 0 to the max allowed time\n",
    "                action = self.act(state, begin) # Calling the action function.\n",
    "                next_state, reward, done = env.step(action) # Calling the step function from the \"grid\" getting reward values for taking steps, getting items etc..\n",
    "                self.step(state, action, reward, next_state, done) # Take actual step based on the above line's values.\n",
    "                state = next_state # Updating the state variable\n",
    "                score += reward # Add the step consequence.\n",
    "                if done: # Done condition is package delivery.\n",
    "                    break\n",
    "\n",
    "            scores_window.append(score) # The last step's reward update is put into the double end queue.\n",
    "            scores.append(score) # Adds the score to the agent's personal record.\n",
    "            begin = max(eps_end, eps_decay * begin) # Reset the begin variable to the max of either the ending episode or decay times begin.\n",
    "\n",
    "            # Display training progress\n",
    "            print(f'\\rEpisode {epis}, Average Score: {np.mean(scores_window):.2f}', end=\"\")\n",
    "\n",
    "            if epis % rolling_epochs == 0:\n",
    "                print(f'\\rEpisode {epis}, Average Score: {np.mean(scores_window):.2f}')\n",
    "\n",
    "            if target_score and np.mean(scores_window) >= target_score:\n",
    "                torch.save(self.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "\n",
    "                    # Plot the scores\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        plt.plot(range(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.show()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Update Q-network parameters using a batch of experience tuples.\n",
    "\n",
    "        Parameters:\n",
    "        ===========\n",
    "            experiences (Tuple[torch.Tensor]): Tuple of (s, a, r, s', done).\n",
    "            gamma (float): Discount factor.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences # Split the tensor into respective variables the named tuple of length 5\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) # Q (expected return) from the target network.\n",
    "        # Detach the current matrix from a tensor, so its not included in updating.\n",
    "        # calculate the maximum of the first 0th element which is the states, carrying their respective rewards from the map\n",
    "        # unsqueeze (grow)_ is just the tensor reshape to add another dimension along the position 1 element\n",
    "        # Giving the next best target for a future calculation.v\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones)) # Best decision given the current rules and move limitation - initialized by the policy.\n",
    "        # Also taking the Q-rewards out of the detached tensor and adding it into the second half of the bellmen equation\n",
    "        # Get expected Q values from local network\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions) # Expected 'rewards' (equal to Q-rewards) out of all next possible states.\n",
    "\n",
    "        # Optimize\n",
    "        loss = F.mse_loss(Q_expected, Q_targets) # Neural network functions, computing the temporal difference error.\n",
    "        # holding the knowledge of the best rewards for an mse loss between expectation and target. Minimizing this difference is the DQ goal.\n",
    "\n",
    "        self.optimizer.zero_grad() # This is like a memory reset for the gradients, so previous runs aren't stored in new runs.\n",
    "        loss.backward() # Do one backward pass through gradients to see the optimum path to best return.\n",
    "        self.optimizer.step() # update \"weights and biases\" through the game board network. According to optimizer set point, Adam in this case.\n",
    "\n",
    "        # Update target network\n",
    "        self.update(self.qnetwork_local, self.qnetwork_target) # This aligns the local network (possible moves) with the target network \"game board\".\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory and learn if enough samples are available.\"\"\"\n",
    "\n",
    "        # Store experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done) #Each agent has its own memory attribute, holding the parameters inside. Now i gotta know what replay buffer is doing.\n",
    "\n",
    "        # Learn from a random subset of experiences if enough samples are available\n",
    "        if len(self.memory) > self.start_sampling: # if theres open memory randomly fill it with samples\n",
    "            experiences = self.memory.sample() # sampling the replay buffer obj from torch\n",
    "            self.learn(experiences, self.gamma) # calls the learn method with above line executed and stored and the set future reward gamma discount rate.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def update(self, local_model, target_model):\n",
    "        \"\"\"Copy weights from local model to target model.\"\"\"\n",
    "        target_model.load_state_dict(local_model.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQb-bdoMbOaa"
   },
   "source": [
    "### Agent Instantiation. Policy learning and Visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "j4d4gVQwbOab",
    "outputId": "d5018ca9-3397-47cd-bb1f-847e71b6b8e4"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m policy \u001b[38;5;241m=\u001b[39m DQN(state_size\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mstate_size, action_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# Use the DQN class to govern agent object.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train agent object with GridWorld environment.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Training process for n_episodes or until desired score is reached.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# over the last 'rolling_epochs' (defaulted to 200 in the DQN class) reaches or exceeds 2.2.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36mDQN.fit\u001b[1;34m(self, env, n_episodes, max_t, rolling_epochs, target_score, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     71\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# Agent's reward score.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t): \u001b[38;5;66;03m# For everything from 0 to the max allowed time\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Calling the action function.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# Calling the step function from the \"grid\" getting reward values for taking steps, getting items etc..\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(state, action, reward, next_state, done) \u001b[38;5;66;03m# Take actual step based on the above line's values.\u001b[39;00m\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36mDQN.act\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# this sets the nn from torch into evaluation mode.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# Disable gradient tracking (local net doesn't learn from the calculation following:::)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnetwork_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# compute the value of taking each act --- then store it.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# return local q net into train mode (learn from acting in XYZ way)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Epsilon-greedy action selection\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Build the neural network that maps state -> action values.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m        x (Tensor): The action values for the given state.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# Initialize a 4x4 (n^2) grid world environment.\n",
    "env = GridWorld(4)\n",
    "\n",
    "# Set agent's personal state size to the environment's state size.\n",
    "# The possible actions in this environment set to 4: up, down, left, right.\n",
    "policy = DQN(state_size=env.state_size, action_size=4) # Use the DQN class to govern agent object.\n",
    "\n",
    "# Train agent object with GridWorld environment.\n",
    "# Training process for n_episodes or until desired score is reached.\n",
    "# over the last 'rolling_epochs' (defaulted to 200 in the DQN class) reaches or exceeds 2.2.\n",
    "policy.fit(env=env, n_episodes=2500, target_score=2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "len(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqRU_2QbbOac"
   },
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8Ld8hlXbOad",
    "outputId": "3eccbacc-8ae3-421d-8ee8-edd14b5abb15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained weights\n",
    "policy.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZ2tDUvZbOae",
    "outputId": "80e7ac03-3cf4-4d1c-b225-a4e9c61e39ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . \n",
      ". . . . \n",
      ". R . . \n",
      ". . . . \n"
     ]
    }
   ],
   "source": [
    "# Visualise 5 episodes using trained agent\n",
    "\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    for j in range(20):\n",
    "        action = policy.act(state)\n",
    "        state, reward, done = env.step(action)\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(1)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waBZHKgnj8FQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
