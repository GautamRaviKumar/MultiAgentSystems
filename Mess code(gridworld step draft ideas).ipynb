{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747e60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d65cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size,action_size = 4):\n",
    "        l1 = state_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = action_size\n",
    "        self.model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(l1, l2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l2, l3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l3,l4))\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# The function \"update_target\" copies the state of the prediction network to the target network. You need to use this in regular intervals.\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# The function \"get_qvals\" returns a numpy list of qvals for the state given by the argument based on the prediction network.\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return q_values\n",
    "\n",
    "# The function \"get_maxQ\" returns the maximum q-value for the state given by the argument based on the target network.\n",
    "    def get_maxQ(self,state):\n",
    "        q_values = self.model(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "# The function \"train_one_step_new\" performs a single training step.\n",
    "# It returns the current loss (only needed for debugging purposes).\n",
    "# Its parameters are three parallel lists: a minibatch of states, a minibatch of actions,\n",
    "# a minibatch of the corresponding TD targets and the discount factor.\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        targets_reply = []\n",
    "        state1_batch = torch.cat([torch.from_numpy(s).float() for s in states])\n",
    "        action_batch = torch.Tensor(actions)\n",
    "        Q1 = self.model(state1_batch)\n",
    "        X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "        Y = torch.tensor(targets).float()\n",
    "        loss = self.loss_fn(X, Y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5caca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, start_position, epsilon):\n",
    "        self.position = position\n",
    "        self.picked = False\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.dqn = initializeDQN()\n",
    "        self.epsilon = epsilon\n",
    "    def action(self, state):\n",
    "\n",
    "        ### INSERT THE DAIRY QUEEN!\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() > epsilon: # zero and 1 random selection number comparison with epsilon\n",
    "            return np.argmax(action_values.cpu().data.numpy())  # should epsilon be smaller, this retrieves the calculated values from processing unit\n",
    "                                                                # puts it into a numpy array. picks the highest reward\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_space)) # randomly makes a choice for its next step.\n",
    "\n",
    "        return np.random.choice([0, 1, 2, 3]) #of the legal moves pic one at random to do.\n",
    "\n",
    "    def next_position(self, action): # HERE the 0-3 is passed in and used.\n",
    "            x_disp, y_disp = {\n",
    "                0: (-1, 0), # up\n",
    "                1: (1, 0), #down\n",
    "                2: (0, -1), #left\n",
    "                3: (0, 1) #right\n",
    "                }[action]\n",
    "\n",
    "            return (min(max(self.start_position[0] + x_disp, 0),self.grid_size[0] - 1),\n",
    "                    min(max(self.start_position[1] + y_disp, 0),self.grid_size[0] - 1))# Here is where the boundary is checked.\n",
    "\n",
    "    def move(self, action): # This actually executes the move. Action is passed in and executed.\n",
    "        self.start_position = self.next_position(action)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.buff.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        self.buff.sample()\n",
    "    def initializeDQN():\n",
    "        return (DQN(state_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73b9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "class MultiAgentGridWorld:\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def __init__(self, size=5, agents_each = 2, eps=0.01):\n",
    "        self.size = size # Grid world size default to 5.\n",
    "        self.pick_up_position = (0, size - 1) # This puts the box in upper left corner of any size map.\n",
    "        self.delivery_position = (size - 1, 0) # The delivery is in lower right corner for any size map.\n",
    "        self.agents_each = agents_each\n",
    "        self.pick_agents = []\n",
    "        self.drop_agents = []\n",
    "        self.agents = self.initialize_agents() #This list of tuples is carrying all agent objects\n",
    "        #the world has these attributes.\n",
    "        self.pick_reward = 10\n",
    "        self.drop_reward = 10\n",
    "        self.handover_reward = 20\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        self.memory = []\n",
    "    def initialize_agents(self):\n",
    "        # Ensuring agents don't spawn on pickup or delivery positions.\n",
    "        possible_positions = [(i, j)\n",
    "                            for i in range(self.size)\n",
    "                            for j in range(self.size)\n",
    "                            if (i, j) not in [self.pick_up_position, self.delivery_position]]\n",
    "        # Add 2 agents of each type\n",
    "        indexes = npr.choice(len(possible_positions), 4, replace=False)\n",
    "        start_positions = [possible_positions[_] for _ in indexes]\n",
    "        for i in range(self.agents_each):\n",
    "            pick_agents.append(Agent(start_positions[i]),eps)\n",
    "            drop_agents.append(Agent(start_positions[i+2]),eps)\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def step(self, actions): # so this method needs a bunch of actions.\n",
    "        state = [[p_a.position for p_a in self.pick_agents],[d_a.position for d_a in self.drop_agents],[p_a.picked for p_a in self.pick_agents],[d_a.picked for d_a in self.drop_agents],[p_a.done for p_a in self.pick_agents],[d_a.done for d_a in self.drop_agents]]\n",
    "        self.steps += 1\n",
    "        next_states = []\n",
    "        actions_p = []\n",
    "        actions_d = []\n",
    "        picked_p = []\n",
    "        picked_d = []\n",
    "        done_p = []\n",
    "        done_d = []\n",
    "        rewards_p = []\n",
    "        rewards_d = []\n",
    "        next_positions_p = []\n",
    "        next_positions_d = []\n",
    "        for agent in self.pick_agents:\n",
    "            action = agent.action(state)\n",
    "            next_pos = agent.next_position(action)\n",
    "            actions_p.append(agent.action(state))\n",
    "            next_positions_p.append(next_pos)\n",
    "        for agent in self.drop_agents:\n",
    "            action = agent.action(state)\n",
    "            next_pos = agent.next_position(action)\n",
    "            actions_d.append(agent.action(state))\n",
    "            next_positions_d.append(next_pos)\n",
    "        self.actions.append(actions_p,actions_d)\n",
    "        for i in range(self.agents_each):\n",
    "            if self.pick_agents[i].done == False:\n",
    "                done_p[i] = self.pick_agents[i].done\n",
    "                if next_positions_p[i] == self.pick_up_position and self.pick_agents[i].picked == False:\n",
    "                    picked_p[i] = True\n",
    "                    rewards_p[i] = self.pick_agents[i].reward + self.pick_reward - self.steps**2\n",
    "                else:\n",
    "                    picked_p[i] = self.pick_agents[i].picked\n",
    "                    rewards_p[i] = self.pick_agents[i].reward\n",
    "                for j in range(self.agents_each):\n",
    "                    if next_positions_p[i] == next_positions_d[j] and self.pick_agents[i].picked == True and self.drop_agents[j].picked == False and self.drop_agents[j].done == False:\n",
    "                        picked_p[i] = False\n",
    "                        picked_d[j] = True\n",
    "                        done_p[i] = True\n",
    "                        rewards_p[i] = self.pick_agents[i].reward + self.handover_reward - self.steps**2\n",
    "                        rewards_d[j] = self.drop_agents[j].reward + self.handover_reward - self.steps**2\n",
    "            else:\n",
    "                done_p[i] = self.pick_agents[i].done\n",
    "                picked_p[i] = self.pick_agents[i].picked\n",
    "                rewards_p[i] = self.pick_agents[i].reward\n",
    "                \n",
    "            if self.drop_agents[i].done == False:\n",
    "                done_d[i] = self.drop_agents[i].done\n",
    "                if next_positions_d[i] == self.delivery_position and self.drop_agents[i].picked == True:\n",
    "                    picked_d[i] = False\n",
    "                    done_d[i] = True\n",
    "                    rewards_d[i] = self.drop_agents[i].reward + self.drop_reward - self.steps**2\n",
    "                else:\n",
    "                    picked_d[i] = self.drop_agents[i].picked\n",
    "                    rewards_d[i] = self.drop_agents[i].reward\n",
    "            else:\n",
    "                done_d[i] = self.drop_agents[i].done\n",
    "                picked_d[i] = self.drop_agents[i].picked\n",
    "                rewards_d[i] = self.drop_agents[i].reward\n",
    "        actions = [actions_p,actions_d] \n",
    "        next_states = [next_positions_p,next_positions_d,picked_p,picked_d,done_p,done_d]\n",
    "        rewards = [rewards_p,rewards_d]\n",
    "        if sum(1 for i in done_p if i == True) == len(done_p) and sum(1 for i in done_d if i == True) == len(done_d):\n",
    "            self.done = True\n",
    "        self.memory.append(state,actions,rewards,next_states, self.done)\n",
    "\n",
    "        #return self.state_of_the_world(), self.done # So each time a step is taken the comeback is these three things.\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def state_of_the_world(self): # This environment only has the spots of agents nd the package picked as its data.\n",
    "        return [a.start_position for a in self.agents], self.has_package\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders grid world.\n",
    "        \"\"\"\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.pick_up_position:\n",
    "                    print(\"P\", end=\" \")\n",
    "                elif (i, j) == self.delivery_position:\n",
    "                    print(\"D\", end=\" \")\n",
    "                elif any(agent.position == (i, j) for agent in pick_agents):\n",
    "                    print(\"1\", end=\" \") #type 1 agent Pickup\n",
    "                elif any(agent.position == (i, j) for agent in drop_agents):\n",
    "                    print('2', end=\" \") #type 2 agent delivery\n",
    "                else:\n",
    "                    print(\".\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    ############################################################################################################################\n",
    "    def reset(self):\n",
    "        self.initialize_agents()\n",
    "        self.package_picked = False\n",
    "        return self.state_of_the_world() #This works because all the method does is set all the world attributes to above.\n",
    "\n",
    "    ############################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a9f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
